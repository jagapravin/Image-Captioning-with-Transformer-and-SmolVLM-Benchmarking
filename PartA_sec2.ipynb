{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a608ec97",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942d95ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T11:45:10.618581Z",
     "iopub.status.busy": "2025-04-11T11:45:10.618354Z",
     "iopub.status.idle": "2025-04-11T11:45:17.019773Z",
     "shell.execute_reply": "2025-04-11T11:45:17.019009Z",
     "shell.execute_reply.started": "2025-04-11T11:45:10.618556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install -U nltk rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e6ff3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T11:45:17.020781Z",
     "iopub.status.busy": "2025-04-11T11:45:17.020587Z",
     "iopub.status.idle": "2025-04-11T11:45:43.726928Z",
     "shell.execute_reply": "2025-04-11T11:45:43.726349Z",
     "shell.execute_reply.started": "2025-04-11T11:45:17.020762Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from PIL import Image\n",
    "from rouge_score import rouge_scorer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoImageProcessor, AutoModel, AutoTokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed6601",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T11:45:43.728838Z",
     "iopub.status.busy": "2025-04-11T11:45:43.728343Z",
     "iopub.status.idle": "2025-04-11T11:45:43.913539Z",
     "shell.execute_reply": "2025-04-11T11:45:43.912940Z",
     "shell.execute_reply.started": "2025-04-11T11:45:43.728818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4656bbd0",
   "metadata": {},
   "source": [
    "### Environment Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d2a60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T11:45:43.914391Z",
     "iopub.status.busy": "2025-04-11T11:45:43.914146Z",
     "iopub.status.idle": "2025-04-11T11:45:43.922696Z",
     "shell.execute_reply": "2025-04-11T11:45:43.922143Z",
     "shell.execute_reply.started": "2025-04-11T11:45:43.914373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42beb407",
   "metadata": {},
   "source": [
    "### Custom Dataset Class for Image Captioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b5322",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T11:45:43.923599Z",
     "iopub.status.busy": "2025-04-11T11:45:43.923383Z",
     "iopub.status.idle": "2025-04-11T11:45:43.930855Z",
     "shell.execute_reply": "2025-04-11T11:45:43.930169Z",
     "shell.execute_reply.started": "2025-04-11T11:45:43.923567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, tokenizer, image_processor, max_length=128):\n",
    "        self.img_captions = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.img_dir, str(self.img_captions.iloc[idx, 1]))\n",
    "        caption = self.img_captions.iloc[idx, 2]\n",
    "\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        pixel_values = self.image_processor(\n",
    "            images=image, return_tensors=\"pt\"\n",
    "        ).pixel_values.squeeze(0)\n",
    "\n",
    "        caption_with_start = f\"<|startoftext|> {caption}\"\n",
    "        caption_encoding = self.tokenizer(\n",
    "            caption_with_start,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = caption_encoding.input_ids.squeeze(0)\n",
    "        attention_mask = caption_encoding.attention_mask.squeeze(0)\n",
    "        target_ids = input_ids.clone()\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"target_ids\": target_ids,\n",
    "            \"caption\": caption,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a772dba",
   "metadata": {},
   "source": [
    "### DataLoader Preparation for Training, Validation, and Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f4fa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T11:45:43.931838Z",
     "iopub.status.busy": "2025-04-11T11:45:43.931592Z",
     "iopub.status.idle": "2025-04-11T11:45:43.951304Z",
     "shell.execute_reply": "2025-04-11T11:45:43.950610Z",
     "shell.execute_reply.started": "2025-04-11T11:45:43.931813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(data_dir, batch_size=16):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    special_tokens = {\"additional_special_tokens\": [\"<|startoftext|>\"]}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\n",
    "        \"WinKawaks/vit-small-patch16-224\"\n",
    "    )\n",
    "\n",
    "    train_dataset = ImageCaptioningDataset(\n",
    "        csv_file=os.path.join(data_dir, \"train.csv\"),\n",
    "        img_dir=os.path.join(data_dir, \"train\"),\n",
    "        tokenizer=tokenizer,\n",
    "        image_processor=image_processor,\n",
    "    )\n",
    "\n",
    "    val_dataset = ImageCaptioningDataset(\n",
    "        csv_file=os.path.join(data_dir, \"val.csv\"),\n",
    "        img_dir=os.path.join(data_dir, \"val\"),\n",
    "        tokenizer=tokenizer,\n",
    "        image_processor=image_processor,\n",
    "    )\n",
    "\n",
    "    test_dataset = ImageCaptioningDataset(\n",
    "        csv_file=os.path.join(data_dir, \"test.csv\"),\n",
    "        img_dir=os.path.join(data_dir, \"test\"),\n",
    "        tokenizer=tokenizer,\n",
    "        image_processor=image_processor,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader, tokenizer, image_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369ef884",
   "metadata": {},
   "source": [
    "### Vision-to-Text Image Captioning Model using ViT Encoder and GPT-2 Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7355584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T11:45:43.952309Z",
     "iopub.status.busy": "2025-04-11T11:45:43.952076Z",
     "iopub.status.idle": "2025-04-11T11:45:43.974213Z",
     "shell.execute_reply": "2025-04-11T11:45:43.973528Z",
     "shell.execute_reply.started": "2025-04-11T11:45:43.952294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageCaptionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vit_model_name=\"WinKawaks/vit-small-patch16-224\",\n",
    "        gpt2_model_name=\"gpt2\",\n",
    "        dropout_rate=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = AutoModel.from_pretrained(vit_model_name)\n",
    "        self.encoder_dim = self.encoder.config.hidden_size\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
    "        self.decoder_dim = self.decoder.config.n_embd\n",
    "\n",
    "        self.image_proj = nn.Sequential(\n",
    "            nn.Linear(self.encoder_dim, self.decoder_dim),\n",
    "            nn.LayerNorm(self.decoder_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.decoder_dim, self.decoder_dim),\n",
    "        )\n",
    "        self.decoder.resize_token_embeddings(self.decoder.config.vocab_size + 1)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        encoder_outputs = self.encoder(pixel_values=pixel_values)\n",
    "        image_embedding = encoder_outputs.last_hidden_state[:, 0]\n",
    "        image_embedding = self.image_proj(image_embedding)\n",
    "        image_embedding = image_embedding.unsqueeze(1)\n",
    "\n",
    "        inputs_embeds = self.decoder.transformer.wte(input_ids)\n",
    "        inputs_embeds = torch.cat([image_embedding, inputs_embeds], dim=1)\n",
    "\n",
    "        extended_attention_mask = torch.cat(\n",
    "            [\n",
    "                torch.ones((attention_mask.size(0), 1), device=attention_mask.device),\n",
    "                attention_mask,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        outputs = self.decoder(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        return outputs.logits\n",
    "\n",
    "    def generate_caption(\n",
    "        self, pixel_values, tokenizer, max_length=128, temperature=0.7\n",
    "    ):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = self.encoder(pixel_values=pixel_values)\n",
    "            image_embedding = encoder_outputs.last_hidden_state[:, 0]\n",
    "            image_embedding = self.image_proj(image_embedding).unsqueeze(1)\n",
    "\n",
    "            generated_ids = torch.tensor(\n",
    "                [[tokenizer.convert_tokens_to_ids(\"<|startoftext|>\")]]\n",
    "            ).to(pixel_values.device)\n",
    "            inputs_embeds = self.decoder.transformer.wte(generated_ids)\n",
    "            inputs_embeds = torch.cat([image_embedding, inputs_embeds], dim=1)\n",
    "\n",
    "            attention_mask = torch.ones(\n",
    "                (inputs_embeds.size(0), inputs_embeds.size(1)),\n",
    "                device=pixel_values.device,\n",
    "            )\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                outputs = self.decoder(\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    attention_mask=attention_mask,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "\n",
    "                next_token_logits = outputs.logits[:, -1, :] / temperature\n",
    "\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > 0.9\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                    1, sorted_indices, sorted_indices_to_remove\n",
    "                )\n",
    "                next_token_logits[indices_to_remove] = -float(\"Inf\")\n",
    "\n",
    "                next_token = torch.multinomial(\n",
    "                    F.softmax(next_token_logits, dim=-1), num_samples=1\n",
    "                )\n",
    "\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "\n",
    "                if next_token.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "                next_token_embeds = self.decoder.transformer.wte(next_token)\n",
    "                inputs_embeds = torch.cat([inputs_embeds, next_token_embeds], dim=1)\n",
    "                attention_mask = torch.cat(\n",
    "                    [attention_mask, torch.ones((1, 1), device=pixel_values.device)],\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "            caption = tokenizer.decode(\n",
    "                generated_ids[0], skip_special_tokens=True\n",
    "            ).strip()\n",
    "            return caption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c43bf2",
   "metadata": {},
   "source": [
    "### Model Training with Early Stopping and Learning Rate Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff98d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T11:45:43.975154Z",
     "iopub.status.busy": "2025-04-11T11:45:43.974931Z",
     "iopub.status.idle": "2025-04-11T11:45:43.997080Z",
     "shell.execute_reply": "2025-04-11T11:45:43.996312Z",
     "shell.execute_reply.started": "2025-04-11T11:45:43.975132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    criterion,\n",
    "    device,\n",
    "    epochs,\n",
    "    early_stopping_patience=10,\n",
    "    save_path=\"best_model.pth\",\n",
    "):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_progress = tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"Epoch {epoch + 1}/{epochs} [Train]\",\n",
    "            position=0,\n",
    "            leave=False,\n",
    "            ncols=100,\n",
    "        )\n",
    "\n",
    "        for batch in train_progress:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "\n",
    "            outputs = model(pixel_values, input_ids, attention_mask)\n",
    "\n",
    "            shift_logits = outputs[:, 1:-1, :].contiguous()\n",
    "            shift_labels = target_ids[:, 1:].contiguous()\n",
    "\n",
    "            loss = criterion(\n",
    "                shift_logits.reshape(-1, shift_logits.size(-1)),\n",
    "                shift_labels.reshape(-1),\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_progress.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{loss.item():.4f}\",\n",
    "                    \"lr\": f\"{optimizer.param_groups[0]['lr']:.6f}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_progress = tqdm(\n",
    "            val_loader,\n",
    "            desc=f\"Epoch {epoch + 1}/{epochs} [Val]\",\n",
    "            position=0,\n",
    "            leave=False,\n",
    "            ncols=100,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            for batch in val_progress:\n",
    "                pixel_values = batch[\"pixel_values\"].to(device)\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                target_ids = batch[\"target_ids\"].to(device)\n",
    "\n",
    "                outputs = model(pixel_values, input_ids, attention_mask)\n",
    "\n",
    "                shift_logits = outputs[:, 1:-1, :].contiguous()\n",
    "                shift_labels = target_ids[:, 1:].contiguous()\n",
    "\n",
    "                loss = criterion(\n",
    "                    shift_logits.reshape(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.reshape(-1),\n",
    "                )\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_progress.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        scheduler.step(avg_val_loss)\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\",\n",
    "            end=\"\",\n",
    "        )\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            patience_counter = 0\n",
    "            print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(\n",
    "                f\"No improvement. Patience: {patience_counter}/{early_stopping_patience}\"\n",
    "            )\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    return model, best_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ca868",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Image Captioning Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d929f9c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T12:11:40.553738Z",
     "iopub.status.busy": "2025-04-11T12:11:40.553458Z",
     "iopub.status.idle": "2025-04-11T12:11:40.562357Z",
     "shell.execute_reply": "2025-04-11T12:11:40.561461Z",
     "shell.execute_reply.started": "2025-04-11T12:11:40.553709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(train_loader, val_loader, tokenizer, device):\n",
    "    model = ImageCaptionModel().to(device)\n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": [5e-5, 1e-4, 5e-4],\n",
    "        \"weight_decay\": [0.01, 0.001],\n",
    "    }\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_hyperparams = None\n",
    "    best_model_state = None\n",
    "    products = list(\n",
    "        product(\n",
    "            hyperparameters[\"learning_rate\"],\n",
    "            hyperparameters[\"weight_decay\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    hp_progress = tqdm(\n",
    "        products,\n",
    "        desc=\"Hyperparameter tuning\",\n",
    "        total=len(products),\n",
    "        position=0,\n",
    "        leave=True,\n",
    "        ncols=100,\n",
    "    )\n",
    "\n",
    "    for lr, wd in hp_progress:\n",
    "        hp_progress.set_description(f\"LR: {lr}, WD: {wd}\")\n",
    "\n",
    "        model = ImageCaptionModel().to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.5, patience=5, verbose=False\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "        model, val_loss = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            epochs=50,\n",
    "            early_stopping_patience=5,\n",
    "            save_path=\"best_image_caption_model.pth\",\n",
    "        )\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_hyperparams = {\n",
    "                \"learning_rate\": lr,\n",
    "                \"weight_decay\": wd,\n",
    "            }\n",
    "            best_model_state = model.state_dict()\n",
    "            hp_progress.set_postfix({\"best_val_loss\": f\"{best_val_loss:.4f}\"})\n",
    "\n",
    "    best_model = ImageCaptionModel()\n",
    "    best_model.load_state_dict(best_model_state)\n",
    "    best_model.to(device)\n",
    "\n",
    "    return best_model, best_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79244619",
   "metadata": {},
   "source": [
    "### Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa29389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T12:11:42.817650Z",
     "iopub.status.busy": "2025-04-11T12:11:42.817399Z",
     "iopub.status.idle": "2025-04-11T12:11:42.826437Z",
     "shell.execute_reply": "2025-04-11T12:11:42.825582Z",
     "shell.execute_reply.started": "2025-04-11T12:11:42.817633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating model\"):\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            original_captions = batch[\"caption\"]\n",
    "\n",
    "            batch_captions = []\n",
    "            for i in range(pixel_values.size(0)):\n",
    "                caption = model.generate_caption(\n",
    "                    pixel_values[i : i + 1],\n",
    "                    tokenizer,\n",
    "                    max_length=128,\n",
    "                    temperature=0.7,\n",
    "                )\n",
    "                batch_captions.append(caption)\n",
    "\n",
    "            for ref_caption, gen_caption in zip(original_captions, batch_captions):\n",
    "                ref_tokens = nltk.word_tokenize(ref_caption.lower())\n",
    "                gen_tokens = nltk.word_tokenize(gen_caption.lower())\n",
    "                references.append([ref_tokens])\n",
    "                hypotheses.append(gen_tokens)\n",
    "\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "\n",
    "    rouge_scores = []\n",
    "    for ref, hyp in zip(\n",
    "        [\" \".join(ref[0]) for ref in references], [\" \".join(hyp) for hyp in hypotheses]\n",
    "    ):\n",
    "        rouge_score = rouge_scorer_obj.score(ref, hyp)\n",
    "        rouge_scores.append(rouge_score[\"rougeL\"].fmeasure)\n",
    "    rouge_l_score = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0\n",
    "\n",
    "    meteor_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        meteor_scores.append(meteor_score(ref, hyp))\n",
    "    meteor_score_avg = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n",
    "\n",
    "    print(\"\\nExample Generations:\")\n",
    "    for i in range(min(5, len(references))):\n",
    "        print(f\"Reference: {' '.join(references[i][0])}\")\n",
    "        print(f\"Generated: {' '.join(hypotheses[i])}\")\n",
    "        print()\n",
    "\n",
    "    results = {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"rouge_l\": rouge_l_score,\n",
    "        \"meteor\": meteor_score_avg,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1406e1",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f39a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T12:11:45.557082Z",
     "iopub.status.busy": "2025-04-11T12:11:45.556551Z",
     "iopub.status.idle": "2025-04-11T12:11:46.174044Z",
     "shell.execute_reply": "2025-04-11T12:11:46.173477Z",
     "shell.execute_reply.started": "2025-04-11T12:11:45.557058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, tokenizer, image_processor = get_dataloaders(\n",
    "    data_dir=\"/kaggle/input/dl-assignment-2/custom_captions_dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc48b77",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Best Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41779b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T12:11:47.210342Z",
     "iopub.status.busy": "2025-04-11T12:11:47.209755Z",
     "iopub.status.idle": "2025-04-11T14:20:19.893327Z",
     "shell.execute_reply": "2025-04-11T14:20:19.892625Z",
     "shell.execute_reply.started": "2025-04-11T12:11:47.210320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_model, best_hyperparams = hyperparameter_tuning(\n",
    "    train_loader, val_loader, tokenizer, device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba8781b",
   "metadata": {},
   "source": [
    "### Storing Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157cae31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T14:20:19.895623Z",
     "iopub.status.busy": "2025-04-11T14:20:19.895360Z",
     "iopub.status.idle": "2025-04-11T14:39:18.209442Z",
     "shell.execute_reply": "2025-04-11T14:39:18.208530Z",
     "shell.execute_reply.started": "2025-04-11T14:20:19.895594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_results = evaluate_model(best_model, test_loader, tokenizer, device)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"BLEU: {test_results['bleu']:.4f}\")\n",
    "print(f\"ROUGE-L: {test_results['rouge_l']:.4f}\")\n",
    "print(f\"METEOR: {test_results['meteor']:.4f}\")\n",
    "\n",
    "torch.save(best_model.state_dict(), \"/kaggle/working/best_image_caption_model.pth\")\n",
    "test_results_df = pd.DataFrame([test_results])\n",
    "test_results_df.to_csv(\"/kaggle/working/custom_results.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6967565,
     "sourceId": 11327601,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
